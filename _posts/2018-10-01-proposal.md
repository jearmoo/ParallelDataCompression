---
title: "proposal"
bg: palm-leaf
color: white
style: left
---

## Summary

We will explore how various lossless compression algorithms can be parallelized,
compare them, and analyze the the effect of parallelization on compression
ratio. We will focus on the Huffman Coding and LZ77 algorithms, attempting to
parallelize using both the CPU and the GPU . We will use the OpenMP API for the
parallel version on the CPU (Xeon Phi Processors), and CUDA for the parallel
version on the GPU (GHC clusters).

## Background

*Lossless* compression algorithms attempt to compress data such that it is
possible to decompress it exactly into the original input, without any loss of
data/reduction in quality. This contrasts *lossy* compression algorithms which can
lose data.

The algorithms that this project attempts to parallelize are two popular
lossless compression algorithms.

The Huffman Coding algorithm uses entropy encoding in its algorithms. This means
that they use less memory to represent characters that occur more frequently at
the expense of using more memory to represent characters that occur more
frequently.

The LZ77 algorithm uses a dictionary encoding algorithm which uses a fixed table
to encode the input. Using a fixed table enables faster
compression/decompression at the expense of lower compression ratio.

The performance of our implementation will be measured by both time taken to
compress an input file and the compression ratio of the file compressed.
Compression ratio is size of
$$\frac{\text{original input}}{\text{size of compressed input}}$$.

## Challenge

All compression algorithms require reading of input data from files and writing
of data to files, which is (but is fairly parallelizable).

The first step of both of our compression algorithms is making a frequency
dictionary. In order to achieve the same compression ratio, every processor must
communicate its frequency dictionary with other processors. This results in a
high communication to computation ratio. Hence, we will have to experiment with
various techniques and determine the tradeoffs between performance and
compression ratio in those techniques.

Moreover, every compression algorithm has 3 main parts to it:
1. Reading from an input file to make a frequency dictionary.
2. Building an encoding format from the input or use a pre-existing format.
3. Encoding the input file using the encoding format.

All the 3 parts are inherently sequential and hence require synchronization to
avoid threads from moving to the next step before all other threads reach the
same point in the program.


## Resources

Shun, Julian, and Fuyao Zhao. "Practical parallel Lempel-Ziv factorization." *Data Compression Conference (DCC), 2013*. IEEE, 20

W. Hu, J. Wen, W. Wu, Y. Han, S. Yang and J. Villasenor, "Highly Scalable Parallel Arithmetic Coding on Multi-Core Processors Using LDPC Codes," in *IEEE Transactions on Communications*, vol. 60, no. 2, pp. 289-294, February 2012.

Youssef, Abdou S., and Abdou Youssef. *Parallel algorithms for entropy-coding
techniques*. US Department of Commerce, Technology Administration, National
Institute of Standards and Technology, 1998.

## Goals/Deliverables

**Plan to Achieve**:
1. Complete sequential and parallel implementations of Huffman Coding and LZ77
algorithms.
2. Achieve a speedup of around 20x with 64 processors, since some existing
libraries/ papers are able to achieve around 15x with 64 processors.
3. Demo will include the performance of the 2 algorithms as processors
increase, the decrease in compression ratio of the 2 algorithms as processors
increase.
4. Graphs showing the % of time taken by each inherently sequential component
in the algorithm.
5. The effect of factors like memory bandwidth, and their effect on the speedup
of the algorithm as the processors increase (this is especially important when
figuring out the idle number of processors to use, while minimising the decrease
in compression ratio).

**Hope to Achieve**:

In addition to what we plan to achieve,
1. Speedup the algorithms by atleast 30x (2x more than other libraries)  when
using 64 processors.
2. Run these compression algorithms on the NVIDIA GPU, and explore the tradeoff
between the higher computation ratio (due to increased processors) vs the higher
communication ratio ( due to copying over the memory from the CPU to the GPU).
3. Compare the effect of memory bottlenecks on the CPU vs the GPU.
4. Complete the sequential and parallel implemenatations for the arithmetic
coding, another entropy encoding algorithm.

## Platform

Language: C++

API: OpenMP, CUDA

Machine: Xeon Phi on the Latedays Cluster, NVIDIA GPU on the GHC Cluster

<!--
ICONS

speedup: fa-tachometer, fa-signal
bigbrain: fa-lightbulb-o

 -->